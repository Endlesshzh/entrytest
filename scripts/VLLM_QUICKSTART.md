# vLLM å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ å¿«é€Ÿå®‰è£…å’Œå¯åŠ¨

### Intel CPU ç”¨æˆ·ï¼ˆæ¨èä½¿ç”¨ CPU æ¨¡å¼ï¼‰

```bash
# 1. å®‰è£… vLLMï¼ˆCPU æ¨¡å¼ï¼‰
# âš ï¸ æ³¨æ„ï¼šä»æºç ç¼–è¯‘ï¼Œå¯èƒ½éœ€è¦ 30-60 åˆ†é’Ÿ
./scripts/setup-vllm.sh --cpu

# 2. å¯åŠ¨æœåŠ¡
./scripts/start-vllm.sh --cpu

# 3. éªŒè¯æœåŠ¡
curl http://localhost:8000/v1/models
```

### GPU ç”¨æˆ·ï¼ˆNVIDIA/Apple Siliconï¼‰

```bash
# 1. å®‰è£… vLLMï¼ˆGPU æ¨¡å¼ï¼‰
./scripts/setup-vllm.sh

# 2. å¯åŠ¨æœåŠ¡
./scripts/start-vllm.sh

# 3. éªŒè¯æœåŠ¡
curl http://localhost:8000/v1/models
```

## ğŸ“ é…ç½®åº”ç”¨

ç¼–è¾‘ `src/main/resources/application.yml`ï¼š

```yaml
llm:
  primary-provider: VLLM
  vllm:
    api-url: http://localhost:8000/v1/chat/completions
    model: meta-llama/Llama-2-7b-chat-hf
```

## ğŸ§ª æµ‹è¯•

```bash
# æµ‹è¯• vLLM API
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [{"role": "user", "content": "Hello"}]
  }'

# æµ‹è¯•åº”ç”¨é›†æˆ
curl -X POST 'http://localhost:8080/api/script/analyze' \
  -H 'Content-Type: application/json' \
  --data-raw '{"script":"def keys = redis.keys(\"user:*\")\nreturn keys"}'
```

## ğŸ“š æ›´å¤šä¿¡æ¯

- **CPU æ¨¡å¼è¯¦ç»†æŒ‡å—**ï¼š`scripts/VLLM_CPU_INSTALL.md`
- **å®Œæ•´æ–‡æ¡£**ï¼š`scripts/README-vllm.md`
- **å¸¸è§é—®é¢˜**ï¼šæŸ¥çœ‹ä¸Šè¿°æ–‡æ¡£çš„ FAQ éƒ¨åˆ†

## âš ï¸ é‡è¦æç¤º

- **CPU æ¨¡å¼**ï¼šæ€§èƒ½æœ‰é™ï¼ˆ20-50 tokens/sï¼‰ï¼Œä»…é€‚ç”¨äºæµ‹è¯•
- **GPU æ¨¡å¼**ï¼šæ€§èƒ½æœ€ä½³ï¼Œæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ
- **æ›¿ä»£æ–¹æ¡ˆ**ï¼šå¦‚æœæ€§èƒ½ä¸è¶³ï¼Œè€ƒè™‘ä½¿ç”¨ Ollama æˆ–äº‘ç«¯ API

