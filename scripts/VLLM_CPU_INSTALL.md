# vLLM CPU æ¨¡å¼å®‰è£…æŒ‡å—ï¼ˆIntel CPUï¼‰

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•åœ¨ Intel CPU ä¸Šå®‰è£…å’Œé…ç½® vLLM çš„ CPU æ¨¡å¼ã€‚

âš ï¸ **é‡è¦æç¤º**ï¼š
- CPU æ¨¡å¼æ€§èƒ½æœ‰é™ï¼šçº¦ 20-50 tokens/s
- ä»…é€‚ç”¨äºè½»é‡çº§æµ‹è¯•å’Œå¼€å‘ç¯å¢ƒ
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ GPU æˆ–äº‘ç«¯ API

## ğŸ”§ ç³»ç»Ÿè¦æ±‚

- **CPU**ï¼šIntel x86_64ï¼ˆæ”¯æŒ AVX-512 æ›´ä½³ï¼‰
- **å†…å­˜**ï¼š16GB+ï¼ˆæ¨è 32GBï¼‰
- **Python**ï¼š3.8 - 3.11
- **ç£ç›˜ç©ºé—´**ï¼š20GB+ï¼ˆç”¨äºæ¨¡å‹å’Œç¼–è¯‘ï¼‰

## ğŸ“¦ å¿«é€Ÿå®‰è£…ï¼ˆæ¨èï¼‰

ä½¿ç”¨é¡¹ç›®æä¾›çš„è„šæœ¬ï¼š

```bash
# 1. è¿›å…¥é¡¹ç›®ç›®å½•
cd /path/to/entrytest

# 2. å®‰è£… vLLMï¼ˆCPU æ¨¡å¼ï¼‰
# âš ï¸ æ³¨æ„ï¼šä»æºç ç¼–è¯‘ï¼Œå¯èƒ½éœ€è¦ 30-60 åˆ†é’Ÿ
./scripts/setup-vllm.sh --cpu

# 3. å¯åŠ¨æœåŠ¡
./scripts/start-vllm.sh --cpu

# 4. éªŒè¯æœåŠ¡
curl http://localhost:8000/v1/models
```

## ğŸ› ï¸ æ‰‹åŠ¨å®‰è£…æ­¥éª¤

### æ­¥éª¤ 1: åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
python3 -m venv vllm-env
source vllm-env/bin/activate
pip install --upgrade pip setuptools wheel
```

### æ­¥éª¤ 2: å®‰è£…ç¼–è¯‘ä¾èµ–

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# å®‰è£… Intel Extension for PyTorchï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰
pip install intel-extension-for-pytorch
```

### æ­¥éª¤ 3: ä»æºç å®‰è£… vLLM

```bash
# å…‹éš† vLLM æºç 
git clone --recursive https://github.com/vllm-project/vllm.git vllm-source
cd vllm-source

# å®‰è£… vLLMï¼ˆCPU æ¨¡å¼ï¼‰
pip install -e . --no-build-isolation

cd ..
```

### æ­¥éª¤ 4: éªŒè¯å®‰è£…

```bash
python3 -m vllm.entrypoints.openai.api_server --help
```

å¦‚æœçœ‹åˆ°å¸®åŠ©ä¿¡æ¯ï¼Œè¯´æ˜å®‰è£…æˆåŠŸã€‚

## ğŸš€ å¯åŠ¨æœåŠ¡

### æ–¹æ³• 1: ä½¿ç”¨è„šæœ¬

```bash
./scripts/start-vllm.sh --cpu
```

### æ–¹æ³• 2: æ‰‹åŠ¨å¯åŠ¨

```bash
source vllm-env/bin/activate

# è®¾ç½®ç¯å¢ƒå˜é‡
export VLLM_USE_CPU=1
export VLLM_CPU_KVCACHE_SPACE=4  # KV ç¼“å­˜ç©ºé—´ï¼ˆGBï¼‰

# å¯åŠ¨æœåŠ¡
python3 -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-2-7b-chat-hf \
  --port 8000 \
  --host 0.0.0.0 \
  --device cpu \
  --max-num-seqs 4
```

## âš™ï¸ é…ç½®å‚æ•°è¯´æ˜

### CPU æ¨¡å¼å…³é”®å‚æ•°

- `--max-num-seqs 4`ï¼šCPU æ¨¡å¼ä¸‹å»ºè®®ä½¿ç”¨è¾ƒå°çš„æ‰¹å¤„ç†å¤§å°
- `VLLM_USE_CPU=1`ï¼šç¯å¢ƒå˜é‡ï¼Œå¯ç”¨ CPU æ¨¡å¼ï¼ˆå¿…éœ€ï¼‰
- `VLLM_CPU_KVCACHE_SPACE=4`ï¼šKV ç¼“å­˜ç©ºé—´ï¼ˆGBï¼‰ï¼Œæ ¹æ®å†…å­˜è°ƒæ•´
- `OMP_NUM_THREADS=8`ï¼šOpenMP çº¿ç¨‹æ•°ï¼Œæ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´
- `MKL_NUM_THREADS=8`ï¼šIntel MKL çº¿ç¨‹æ•°ï¼Œæ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´

### æ€§èƒ½ä¼˜åŒ–å‚æ•°

```bash
# ä½¿ç”¨ Intel Extension for PyTorch ä¼˜åŒ–
export IPEX_TORCH_CPU_LAUNCH=1

# è®¾ç½®çº¿ç¨‹æ•°ï¼ˆæ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´ï¼‰
export OMP_NUM_THREADS=8
export MKL_NUM_THREADS=8
```

## ğŸ“Š æ€§èƒ½å‚è€ƒ

| é…ç½® | æ€§èƒ½ | è¯´æ˜ |
|------|------|------|
| Intel CPUï¼ˆæ— ä¼˜åŒ–ï¼‰ | 20-30 tokens/s | åŸºç¡€æ€§èƒ½ |
| Intel CPU + IPEX | 30-50 tokens/s | ä½¿ç”¨ Intel æ‰©å±•ä¼˜åŒ– |
| GPUï¼ˆå‚è€ƒï¼‰ | 100+ tokens/s | GPU æ¨¡å¼æ€§èƒ½ |

## ğŸ§ª æµ‹è¯•æœåŠ¡

### 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€

```bash
curl http://localhost:8000/v1/models
```

### 2. æµ‹è¯• API

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 100
  }'
```

### 3. æµ‹è¯•åº”ç”¨é›†æˆ

```bash
# ç¡®ä¿ application.yml ä¸­é…ç½®äº† vLLM
# primary-provider: VLLM

curl -X POST 'http://localhost:8080/api/script/analyze' \
  -H 'Content-Type: application/json' \
  --data-raw '{"script":"def keys = redis.keys(\"user:*\")\nreturn keys"}'
```

## âŒ å¸¸è§é—®é¢˜

### 1. ç¼–è¯‘å¤±è´¥

**é—®é¢˜**ï¼š`error: failed to build wheel`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# ç¡®ä¿å®‰è£…äº†æ‰€æœ‰ç¼–è¯‘å·¥å…·
# macOS
xcode-select --install

# Linux
sudo apt-get install build-essential cmake
```

### 2. å†…å­˜ä¸è¶³

**é—®é¢˜**ï¼š`Out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ 7Bï¼‰
- å‡å°‘ `VLLM_CPU_KVCACHE_SPACE` å€¼
- å‡å°‘ `--max-num-seqs` å‚æ•°

### 3. æ€§èƒ½å¤ªä½

**é—®é¢˜**ï¼šæ¨ç†é€Ÿåº¦å¾ˆæ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š
- è¿™æ˜¯ CPU æ¨¡å¼çš„æ­£å¸¸è¡¨ç°
- å®‰è£… Intel Extension for PyTorch
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹
- è€ƒè™‘ä½¿ç”¨ GPU æˆ–äº‘ç«¯ API

### 4. æ¨¡å‹ä¸‹è½½å¤±è´¥

**é—®é¢˜**ï¼šæ— æ³•ä¸‹è½½æ¨¡å‹

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
# ç„¶åä½¿ç”¨æœ¬åœ°è·¯å¾„
python3 -m vllm.entrypoints.openai.api_server \
  --model /path/to/local/model \
  --device cpu
```

## ğŸ”„ ä¸é¡¹ç›®é›†æˆ

### æ›´æ–° application.yml

ç¡®ä¿é…ç½®äº† vLLMï¼š

```yaml
llm:
  primary-provider: VLLM
  vllm:
    api-url: http://localhost:8000/v1/chat/completions
    model: meta-llama/Llama-2-7b-chat-hf
```

### åˆ‡æ¢æä¾›å•†

å¦‚æœéœ€è¦åˆ‡æ¢å›å…¶ä»–æä¾›å•†ï¼š

```yaml
llm:
  primary-provider: OLLAMA  # æˆ–å…¶ä»–æä¾›å•†
```

## ğŸ“š å‚è€ƒèµ„æº

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [vLLM CPU å®‰è£…æŒ‡å—](https://docs.vllm.ai/en/latest/getting_started/installation/cpu.html)
- [Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/)

## ğŸ’¡ æ›¿ä»£æ–¹æ¡ˆ

å¦‚æœ CPU æ¨¡å¼æ€§èƒ½æ— æ³•æ»¡è¶³éœ€æ±‚ï¼Œè€ƒè™‘ï¼š

1. **Ollama**ï¼šæ›´é€‚åˆ CPU çš„æœ¬åœ° LLM æ–¹æ¡ˆ
2. **äº‘ç«¯ API**ï¼šOpenAIã€Claude ç­‰
3. **GPU æœåŠ¡å™¨**ï¼šä½¿ç”¨äº‘ GPU æœåŠ¡

