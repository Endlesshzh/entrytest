# å¤§æ¨¡å‹éƒ¨ç½²æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•éƒ¨ç½²å’Œé…ç½®å„ç§å¤§æ¨¡å‹æä¾›å•†ã€‚

## ğŸ“‹ æ”¯æŒçš„å¤§æ¨¡å‹

| æä¾›å•† | ç±»å‹ | æˆæœ¬ | æ€§èƒ½ | éšç§ | æ¨èåœºæ™¯ |
|--------|------|------|------|------|----------|
| **OpenAI** | äº‘ç«¯ | ğŸ’°ğŸ’°ğŸ’° | â­â­â­â­â­ | â­â­ | ç”Ÿäº§ç¯å¢ƒï¼Œé«˜è´¨é‡åˆ†æ |
| **Claude** | äº‘ç«¯ | ğŸ’°ğŸ’°ğŸ’° | â­â­â­â­â­ | â­â­ | ç”Ÿäº§ç¯å¢ƒï¼Œä»£ç åˆ†æ |
| **Compass** | äº‘ç«¯ | ğŸ’°ğŸ’° | â­â­â­â­ | â­â­â­ | å›½å†…éƒ¨ç½²ï¼Œåˆè§„è¦æ±‚ |
| **Ollama** | æœ¬åœ° | ğŸ’° | â­â­â­ | â­â­â­â­â­ | å¼€å‘ç¯å¢ƒï¼Œéšç§ä¿æŠ¤ |
| **vLLM** | æœ¬åœ° | ğŸ’° | â­â­â­â­ | â­â­â­â­â­ | ç”Ÿäº§ç¯å¢ƒï¼Œé«˜æ€§èƒ½ |

## 1ï¸âƒ£ OpenAI éƒ¨ç½²

### è·å–API Key

1. è®¿é—® https://platform.openai.com/
2. æ³¨å†Œ/ç™»å½•è´¦å·
3. è¿›å…¥ API Keys é¡µé¢
4. åˆ›å»ºæ–°çš„ API Key

### é…ç½®

```yaml
llm:
  primary-provider: OPENAI
  openai:
    api-key: sk-...  # ä½ çš„API Key
    model: gpt-3.5-turbo  # æˆ– gpt-4
```

### ç¯å¢ƒå˜é‡æ–¹å¼

```bash
export OPENAI_API_KEY="sk-..."
```

### æµ‹è¯•è¿æ¥

```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

### æ¨¡å‹é€‰æ‹©

- **gpt-3.5-turbo**: å¿«é€Ÿã€ä¾¿å®œï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯
- **gpt-4**: æ›´å¼ºå¤§ï¼Œé€‚åˆå¤æ‚åˆ†æ
- **gpt-4-turbo**: æ€§ä»·æ¯”é«˜ï¼Œæ¨è

### æˆæœ¬ä¼°ç®—

- gpt-3.5-turbo: $0.0015/1K tokens (è¾“å…¥) + $0.002/1K tokens (è¾“å‡º)
- gpt-4: $0.03/1K tokens (è¾“å…¥) + $0.06/1K tokens (è¾“å‡º)

æ¯æ¬¡åˆ†æçº¦æ¶ˆè€— 500-1000 tokensï¼Œæˆæœ¬çº¦ $0.001-0.05

## 2ï¸âƒ£ Claude éƒ¨ç½²

### è·å–API Key

1. è®¿é—® https://console.anthropic.com/
2. æ³¨å†Œ/ç™»å½•è´¦å·
3. è·å– API Key

### é…ç½®

```yaml
llm:
  primary-provider: CLAUDE
  claude:
    api-key: sk-ant-...
    model: claude-3-sonnet-20240229
```

### ç¯å¢ƒå˜é‡æ–¹å¼

```bash
export CLAUDE_API_KEY="sk-ant-..."
```

### æµ‹è¯•è¿æ¥

```bash
curl https://api.anthropic.com/v1/messages \
  -H "x-api-key: $CLAUDE_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -H "content-type: application/json" \
  -d '{
    "model": "claude-3-sonnet-20240229",
    "max_tokens": 1024,
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

### æ¨¡å‹é€‰æ‹©

- **claude-3-haiku**: æœ€å¿«æœ€ä¾¿å®œ
- **claude-3-sonnet**: å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼ˆæ¨èï¼‰
- **claude-3-opus**: æœ€å¼ºå¤§

## 3ï¸âƒ£ Compass (å›½å†…å¤§æ¨¡å‹) éƒ¨ç½²

### æ”¯æŒçš„å›½å†…å¤§æ¨¡å‹

- **æ–‡å¿ƒä¸€è¨€** (ç™¾åº¦)
- **é€šä¹‰åƒé—®** (é˜¿é‡Œ)
- **è®¯é£æ˜Ÿç«**
- **æ™ºè°±AI**

### æ–‡å¿ƒä¸€è¨€é…ç½®ç¤ºä¾‹

```yaml
llm:
  primary-provider: COMPASS
  compass:
    api-url: https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions
    api-key: your-access-token
    model: ERNIE-Bot
```

### è·å–Access Token

```bash
# ç™¾åº¦æ–‡å¿ƒä¸€è¨€
curl -X POST \
  'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=YOUR_API_KEY&client_secret=YOUR_SECRET_KEY'
```

### é€šä¹‰åƒé—®é…ç½®

```yaml
llm:
  compass:
    api-url: https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation
    api-key: sk-...
    model: qwen-turbo
```

## 4ï¸âƒ£ Ollama (æœ¬åœ°éƒ¨ç½²)

### å®‰è£…

#### macOS
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### Windows
ä¸‹è½½å®‰è£…åŒ…: https://ollama.com/download

### å¯åŠ¨æœåŠ¡

```bash
ollama serve
```

### æ‹‰å–æ¨¡å‹

```bash
# æ¨èæ¨¡å‹
ollama pull llama2          # é€šç”¨æ¨¡å‹
ollama pull codellama       # ä»£ç åˆ†æä¸“ç”¨
ollama pull mistral         # è½»é‡çº§
ollama pull qwen            # ä¸­æ–‡ä¼˜åŒ–

# æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
ollama list
```

### é…ç½®

```yaml
llm:
  primary-provider: OLLAMA
  ollama:
    api-url: http://localhost:11434/api/generate
    model: llama2
```

### æµ‹è¯•

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Hello, world!",
  "stream": false
}'
```

### æ¨¡å‹æ¨è

| æ¨¡å‹ | å¤§å° | å†…å­˜éœ€æ±‚ | é€Ÿåº¦ | è´¨é‡ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|------|------|----------|
| llama2 | 7B | 8GB | â­â­â­ | â­â­â­â­ | é€šç”¨ |
| codellama | 7B | 8GB | â­â­â­ | â­â­â­â­â­ | ä»£ç åˆ†æ |
| mistral | 7B | 8GB | â­â­â­â­ | â­â­â­â­ | å¿«é€Ÿå“åº” |
| qwen | 7B | 8GB | â­â­â­ | â­â­â­â­ | ä¸­æ–‡åœºæ™¯ |

## 5ï¸âƒ£ vLLM (é«˜æ€§èƒ½æœ¬åœ°éƒ¨ç½²)

### å®‰è£…

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv vllm-env
source vllm-env/bin/activate

# å®‰è£…vLLM
pip install vllm

# æˆ–ä½¿ç”¨conda
conda create -n vllm python=3.10
conda activate vllm
pip install vllm
```

### å¯åŠ¨æœåŠ¡

```bash
# åŸºç¡€å¯åŠ¨
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-2-7b-chat-hf \
  --port 8000

# é«˜æ€§èƒ½é…ç½®
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-2-7b-chat-hf \
  --port 8000 \
  --tensor-parallel-size 2 \
  --gpu-memory-utilization 0.9 \
  --max-num-seqs 256
```

### é…ç½®

```yaml
llm:
  primary-provider: VLLM
  vllm:
    api-url: http://localhost:8000/v1/chat/completions
    model: meta-llama/Llama-2-7b-chat-hf
```

### æµ‹è¯•

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

### æ€§èƒ½ä¼˜åŒ–

```bash
# ä½¿ç”¨å¤šGPU
--tensor-parallel-size 4

# ä¼˜åŒ–å†…å­˜ä½¿ç”¨
--gpu-memory-utilization 0.95

# å¢åŠ å¹¶å‘
--max-num-seqs 512

# å¯ç”¨é‡åŒ–
--quantization awq
```

### æ¨èæ¨¡å‹

- **Llama-2-7b-chat-hf**: å¹³è¡¡æ€§èƒ½
- **Llama-2-13b-chat-hf**: æ›´å¥½è´¨é‡
- **CodeLlama-7b-Instruct-hf**: ä»£ç åˆ†æ
- **Mistral-7B-Instruct-v0.2**: é«˜æ€§èƒ½

## 6ï¸âƒ£ LMDeploy (å¯é€‰)

### å®‰è£…

```bash
pip install lmdeploy
```

### å¯åŠ¨

```bash
lmdeploy serve api_server \
  --model-path /path/to/model \
  --server-port 8000
```

### é…ç½®

ä½¿ç”¨ä¸vLLMç›¸åŒçš„é…ç½®æ ¼å¼ï¼ˆOpenAIå…¼å®¹ï¼‰

## 7ï¸âƒ£ SGLang (å¯é€‰)

### å®‰è£…

```bash
pip install sglang
```

### å¯åŠ¨

```bash
python -m sglang.launch_server \
  --model-path meta-llama/Llama-2-7b-chat-hf \
  --port 8000
```

## ğŸ”„ åˆ‡æ¢å¤§æ¨¡å‹

### è¿è¡Œæ—¶åˆ‡æ¢

ä¿®æ”¹ `application.yml`:

```yaml
llm:
  primary-provider: OPENAI  # æ”¹ä¸ºå…¶ä»–æä¾›å•†
```

é‡å¯åº”ç”¨å³å¯ã€‚

### å¤šæä¾›å•†é…ç½®

å¯ä»¥åŒæ—¶é…ç½®å¤šä¸ªæä¾›å•†ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨fallbackï¼š

```yaml
llm:
  primary-provider: OPENAI

  openai:
    api-key: sk-...

  ollama:
    api-url: http://localhost:11434/api/generate
```

å¦‚æœOpenAIä¸å¯ç”¨ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨Ollamaã€‚

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æä¾›å•† | å“åº”æ—¶é—´ | QPS | æˆæœ¬/1Kè¯·æ±‚ |
|--------|----------|-----|-------------|
| OpenAI | 2-5s | 50 | $1-5 |
| Claude | 2-5s | 50 | $1-5 |
| Compass | 3-6s | 30 | Â¥5-20 |
| Ollama | 5-15s | 10 | $0 |
| vLLM | 1-3s | 100+ | $0 |

## ğŸ¯ é€‰æ‹©å»ºè®®

### ç”Ÿäº§ç¯å¢ƒ

**é«˜è´¨é‡è¦æ±‚**:
- é¦–é€‰: OpenAI (gpt-4) æˆ– Claude (opus)
- å¤‡é€‰: vLLM (å¤§æ¨¡å‹)

**æˆæœ¬æ•æ„Ÿ**:
- é¦–é€‰: vLLM
- å¤‡é€‰: OpenAI (gpt-3.5-turbo)

**å›½å†…éƒ¨ç½²**:
- é¦–é€‰: Compass (æ–‡å¿ƒä¸€è¨€/é€šä¹‰åƒé—®)
- å¤‡é€‰: vLLM

### å¼€å‘ç¯å¢ƒ

- é¦–é€‰: Ollama (å…è´¹ã€éšç§)
- å¤‡é€‰: OpenAI (gpt-3.5-turbo)

### éšç§è¦æ±‚é«˜

- å¿…é€‰: Ollama æˆ– vLLM (æœ¬åœ°éƒ¨ç½²)

## ğŸ”§ æ•…éšœæ’æŸ¥

### OpenAIè¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥API Key
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"

# æ£€æŸ¥ç½‘ç»œ
ping api.openai.com
```

### Ollamaæ— æ³•å¯åŠ¨

```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :11434

# æŸ¥çœ‹æ—¥å¿—
ollama serve --debug

# é‡å¯æœåŠ¡
pkill ollama
ollama serve
```

### vLLMå†…å­˜ä¸è¶³

```bash
# å‡å°‘GPUå†…å­˜ä½¿ç”¨
--gpu-memory-utilization 0.7

# ä½¿ç”¨é‡åŒ–æ¨¡å‹
--quantization awq

# å‡å°‘å¹¶å‘
--max-num-seqs 64
```

## ğŸ“š å‚è€ƒèµ„æº

- OpenAIæ–‡æ¡£: https://platform.openai.com/docs
- Claudeæ–‡æ¡£: https://docs.anthropic.com/
- Ollamaæ–‡æ¡£: https://ollama.com/docs
- vLLMæ–‡æ¡£: https://docs.vllm.ai/
- æ–‡å¿ƒä¸€è¨€: https://cloud.baidu.com/doc/WENXINWORKSHOP/
- é€šä¹‰åƒé—®: https://help.aliyun.com/zh/dashscope/
