server:
  port: 8080

spring:
  application:
    name: redis-script-query-system

  # DevTools Configuration - 完全禁用热重启
  devtools:
    restart:
      enabled: false

  # Redis Configuration
  redis:
    host: localhost
    port: 6379
    password:
    database: 0
    lettuce:
      pool:
        max-active: 8
        max-idle: 8
        min-idle: 0
        max-wait: -1ms
    timeout: 3000ms

# LLM Configuration - Multiple Providers Support
llm:
  # Primary provider: OPENAI, CLAUDE, COMPASS, OLLAMA, VLLM
  # 主提供商：OPENAI（OpenAI官方API）、CLAUDE（Anthropic官方API）、COMPASS（Shopee Compass OpenAI API Proxy）、OLLAMA（本地部署）、VLLM（本地高性能部署）
  primary-provider: OLLAMA

  # Request timeout in seconds (increased for Ollama local models)
  timeout: 180
  # Temperature for generation (0.0 - 1.0)
  temperature: 0.3
  # Max tokens to generate
  max-tokens: 2000

  # OpenAI Configuration (通过 Compass 代理)
  # 使用 Compass OpenAI API Proxy
  # 文档: https://compass.llm.shopee.io/docs/PLATFORM/openai-api-proxy
  openai:
    api-url: http://compass.llm.shopee.io/compass-api/v1/chat/completions
    api-key: 167cf909df74b9896264802b6beb5ba41beb3ba651880127576d03d776071afb
    model: gpt-3.5-turbo  # 可选: gpt-3.5-turbo, gpt-4, gpt-4-turbo

  # Claude Configuration (通过 Compass 代理)
  # 使用 Compass Claude API Proxy
  # 文档: https://compass.llm.shopee.io/docs/PLATFORM/openai-api-proxy
  # 注意：模型名称需要在 Compass 平台配置，请查看 Compass 文档获取正确的模型名称
  claude:
    api-url: http://compass.llm.shopee.io/compass-api/v1/chat/completions
    api-key: 167cf909df74b9896264802b6beb5ba41beb3ba651880127576d03d776071afb
    model: gpt-oss-120b

  # Compass Configuration (Shopee Compass OpenAI API Proxy)
  # 通过 Compass 的 OpenAI 兼容代理接入，支持统一接口调用多种模型
  # 文档: https://compass.llm.shopee.io/docs/PLATFORM/openai-api-proxy
  # 注意：模型名称需要在 Compass 平台配置，请查看 Compass 文档获取正确的模型名称
  compass:
    api-url: http://compass.llm.shopee.io/compass-api/v1/chat/completions
    api-key: 167cf909df74b9896264802b6beb5ba41beb3ba651880127576d03d776071afb
    model: DeepSeek-R1

  # Ollama Configuration (Local)
  ollama:
    api-url: http://localhost:11434/api/generate
    model: llama2

  # vLLM Configuration (Local Deployment)
  vllm:
    api-url: http://localhost:8000/v1/chat/completions
    model: meta-llama/Llama-2-7b-chat-hf

# Script Engine Configuration
script:
  # Maximum script execution time in milliseconds
  max-execution-time: 5000
  # Enable/disable script caching
  cache-enabled: true
  # Maximum number of cached scripts
  cache-size: 100
  # Allowed Redis commands (whitelist)
  allowed-commands:
    - GET
    - SET
    - HGET
    - HGETALL
    - HSET
    - KEYS
    - SCAN
    - MGET
    - LRANGE
    - SMEMBERS
    - ZRANGE
    - TTL
    - EXISTS
  # Forbidden patterns in scripts
  forbidden-patterns:
    - "FLUSHDB"
    - "FLUSHALL"
    - "SHUTDOWN"
    - "CONFIG"
    - "SCRIPT KILL"
    - "SCRIPT FLUSH"
    - "BGREWRITEAOF"
    - "BGSAVE"
    - "SAVE"
    - "DEBUG"
    - "MIGRATE"

# Logging Configuration
logging:
  level:
    root: INFO
    org.example: DEBUG
    org.example.service: DEBUG
    org.example.service.llm: DEBUG
    org.example.performance: INFO
    org.springframework: INFO
    org.springframework.web: DEBUG
    org.springframework.data.redis: INFO

# Actuator for monitoring and metrics
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus,loggers
  metrics:
    export:
      prometheus:
        enabled: true
    tags:
      application: redis-script-query-system
  tracing:
    sampling:
      probability: 1.0
